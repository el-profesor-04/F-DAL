{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4b99c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets.folder import default_loader\n",
    "from torchvision import transforms\n",
    "import torch.nn\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Function\n",
    "import torch.optim as optim\n",
    "\n",
    "from typing import Optional, Any, Tuple\n",
    "import torch.nn.utils.spectral_norm as sn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b2fb4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import fire\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a1b231f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(img_file, label_file, txt_file, n_images):\n",
    "    print(\"\\nOpening binary pixels and labels files \")\n",
    "    lbl_f = open(label_file, \"rb\")   # labels (digits)\n",
    "    img_f = open(img_file, \"rb\")     # pixel values\n",
    "    print(\"Opening destination text file \")\n",
    "    txt_f = open(txt_file, \"w\")      # output to write to\n",
    "\n",
    "    print(\"Discarding binary pixel and label headers \")\n",
    "    img_f.read(16)   # discard header info\n",
    "    lbl_f.read(8)    # discard header info\n",
    "\n",
    "    print(\"\\nReading binary files, writing to text file \")\n",
    "    print(\"Format: 784 pixels then labels, tab delimited \")\n",
    "    for i in range(n_images):   # number requested \n",
    "        lbl = ord(lbl_f.read(1))  # Unicode, one byte\n",
    "        for j in range(784):  # get 784 pixel vals\n",
    "            val = ord(img_f.read(1))\n",
    "            txt_f.write(str(val) + \"\\t\") \n",
    "        txt_f.write(str(lbl) + \"\\n\")\n",
    "    img_f.close(); txt_f.close(); lbl_f.close()\n",
    "    print(\"\\nDone \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6adb912f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "path = \"./usps.h5\"\n",
    "with h5py.File(path, 'r') as hf:\n",
    "        train = hf.get('train')\n",
    "        X_tr = train.get('data')[:]\n",
    "        y_tr = train.get('target')[:]\n",
    "        test = hf.get('test')\n",
    "        X_te = test.get('data')[:]\n",
    "        y_te = test.get('target')[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc067558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Opening binary pixels and labels files \n",
      "Opening destination text file \n",
      "Discarding binary pixel and label headers \n",
      "\n",
      "Reading binary files, writing to text file \n",
      "Format: 784 pixels then labels, tab delimited \n",
      "\n",
      "Done \n"
     ]
    }
   ],
   "source": [
    "convert(\"train-images.idx3-ubyte.bin\",\n",
    "          \"train-labels.idx1-ubyte.bin\",\n",
    "          \"mnist_train.txt\", 42000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10346120",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr*=255\n",
    "X_tr = X_tr.astype('uint8')\n",
    "X_te*=255\n",
    "X_te = X_te.astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "134215f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_txt = open(\"usps_train.txt\", \"w\")\n",
    "for i in range(len(X_tr)):\n",
    "    lbl = y_tr[i]\n",
    "    for j in range(len(X_tr[i])):\n",
    "        val = X_tr[i][j]\n",
    "        train_txt.write(str(val) + \"\\t\")\n",
    "    train_txt.write(str(lbl) + \"\\n\")\n",
    "train_txt.close()\n",
    "test_txt = open(\"usps_test.txt\", \"w\")\n",
    "for i in range(len(X_te)):\n",
    "    lbl = y_te[i]\n",
    "    for j in range(len(X_te[i])):\n",
    "        val = X_te[i][j]\n",
    "        test_txt.write(str(val) + \"\\t\")\n",
    "    test_txt.write(str(lbl) + \"\\n\")\n",
    "test_txt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "07453130",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_txt = open(\"usps_img_train.txt\", \"w\")\n",
    "for i in range(len(X_tr)):\n",
    "    lbl = y_tr[i]\n",
    "    plt.imsave(\"./usps/train\"+str(i)+\".png\", X_tr[i].reshape((16,16)), cmap='gray')\n",
    "    train_txt.write(\"./usps/train\"+str(i)+\".png\" + \"\\t\")\n",
    "    train_txt.write(str(lbl) + \"\\n\")\n",
    "train_txt.close()\n",
    "\n",
    "test_txt = open(\"usps_img_test.txt\", \"w\")\n",
    "for i in range(len(X_te)):\n",
    "    lbl = y_te[i]\n",
    "    plt.imsave(\"./usps/test\"+str(i)+\".png\", X_te[i].reshape((16,16)), cmap=\"gray\")\n",
    "    test_txt.write(\"./usps/test\"+str(i)+\".png\" + \"\\t\")\n",
    "    test_txt.write(str(lbl) + \"\\n\")\n",
    "test_txt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a0e2ff17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_img_tf(img_file, label_file, txt_file, n_images):\n",
    "    print(\"\\nOpening binary pixels and labels files \")\n",
    "    lbl_f = open(label_file, \"rb\")   # labels (digits)\n",
    "    img_f = open(img_file, \"rb\")     # pixel values\n",
    "    print(\"Opening destination text file \")\n",
    "    txt_f = open(txt_file, \"w\")      # output to write to\n",
    "\n",
    "    print(\"Discarding binary pixel and label headers \")\n",
    "    img_f.read(16)   # discard header info\n",
    "    lbl_f.read(8)    # discard header info\n",
    "\n",
    "    print(\"\\nReading binary files, writing to text file \")\n",
    "    print(\"Format: 784 pixels then labels, tab delimited \")\n",
    "    for i in range(n_images):   # number requested \n",
    "        lbl = ord(lbl_f.read(1))  # Unicode, one byte\n",
    "        temp = []\n",
    "        for j in range(784):  # get 784 pixel vals\n",
    "            val = ord(img_f.read(1))\n",
    "            temp.append(val)\n",
    "        temp = np.array(temp)\n",
    "        temp = temp.reshape((28,28))\n",
    "        plt.imsave(\"./mnist/train\"+str(i)+\".png\", temp, cmap='gray')\n",
    "        txt_f.write(\"./mnist/train\"+str(i)+\".png\" + \"\\t\")\n",
    "        txt_f.write(str(lbl) + \"\\n\")\n",
    "    img_f.close(); txt_f.close(); lbl_f.close()\n",
    "    print(\"\\nDone \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "597bdd81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Opening binary pixels and labels files \n",
      "Opening destination text file \n",
      "Discarding binary pixel and label headers \n",
      "\n",
      "Reading binary files, writing to text file \n",
      "Format: 784 pixels then labels, tab delimited \n",
      "\n",
      "Done \n"
     ]
    }
   ],
   "source": [
    "convert_img_tf(\"train-images.idx3-ubyte.bin\",\n",
    "          \"train-labels.idx1-ubyte.bin\",\n",
    "          \"mnist_img_train.txt\", 42000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6dd6bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabb0fd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8646b34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc1ba99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d44e92f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data_loaders():\n",
    "    source_list = 'mnist_img_train.txt'\n",
    "    target_list = 'usps_img_train.txt'\n",
    "    test_list = 'usps_img_test.txt'\n",
    "    batch_size = 128\n",
    "\n",
    "    # training loaders....\n",
    "    train_source = torch.utils.data.DataLoader(\n",
    "        ImageList(open(source_list).readlines(), transform=transforms.Compose([\n",
    "            transforms.Resize((28, 28)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,), (0.5,))\n",
    "        ]), mode='L'),\n",
    "        batch_size=batch_size, shuffle=True, num_workers=8, drop_last=True, pin_memory=True)\n",
    "\n",
    "    train_target = torch.utils.data.DataLoader(\n",
    "        ImageList(open(target_list).readlines(), transform=transforms.Compose([\n",
    "            transforms.Resize((28, 28)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,), (0.5,))\n",
    "        ]), mode='L'),\n",
    "        batch_size=batch_size, shuffle=True, num_workers=8, drop_last=True, pin_memory=True)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        ImageList(open(test_list).readlines(), transform=transforms.Compose([\n",
    "            transforms.Resize((28, 28)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,), (0.5,))\n",
    "        ]), mode='L'),\n",
    "        batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "\n",
    "    return train_source, train_target, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45785b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageList(Dataset):\n",
    "    def __init__(self,image_list, labels=None, transform=None, target_transform=None, mode='RGB'):\n",
    "        imgs = make_dataset(image_list, labels)\n",
    "        if len(imgs) == 0:\n",
    "            raise RuntimeError(\"Images not found\")\n",
    "\n",
    "        self.imgs = imgs\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        if mode == 'RGB':\n",
    "            self.loader = rgb_loader\n",
    "        elif mode == 'L':\n",
    "            self.loader = l_loader\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path, target = self.imgs[index]\n",
    "        img = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5a218d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb_loader(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        with Image.open(f) as img:\n",
    "            return img.convert('RGB')\n",
    "\n",
    "\n",
    "def l_loader(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        with Image.open(f) as img:\n",
    "            return img.convert('L')\n",
    "def make_dataset(image_list, labels):\n",
    "    if labels:\n",
    "        len_ = len(image_list)\n",
    "        images = [(image_list[i].strip(), labels[i, :]) for i in range(len_)]\n",
    "    else:\n",
    "        if len(image_list[0].split()) > 2:\n",
    "            images = [(val.split()[0], np.array([int(la) for la in val.split()[1:]])) for val in image_list]\n",
    "        else:\n",
    "            images = [(val.split()[0], int(val.split()[1])) for val in image_list]\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c34d3ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_all(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "def build_network():\n",
    "    # network encoder...\n",
    "    lenet = nn.Sequential(\n",
    "        nn.Conv2d(1, 20, kernel_size=5),\n",
    "        nn.MaxPool2d(2),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(20, 50, kernel_size=5),\n",
    "        nn.Dropout2d(p=0.5),\n",
    "        nn.MaxPool2d(2),\n",
    "        nn.ReLU(),\n",
    "        nn.Flatten(),\n",
    "    )\n",
    "\n",
    "    # create a bootleneck layer. it usually helps\n",
    "    bottleneck_dim = 500\n",
    "    bottleneck = nn.Sequential(\n",
    "        nn.Linear(800, bottleneck_dim),\n",
    "        nn.BatchNorm1d(bottleneck_dim),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Dropout(0.5)\n",
    "    )\n",
    "\n",
    "    backbone = nn.Sequential(\n",
    "        lenet,\n",
    "        bottleneck\n",
    "    )\n",
    "\n",
    "    # classification head\n",
    "    num_classes = 10\n",
    "    taskhead = nn.Sequential(\n",
    "        sn(nn.Linear(bottleneck_dim, bottleneck_dim)),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "        sn(nn.Linear(bottleneck_dim, num_classes)),\n",
    "    )\n",
    "\n",
    "    return backbone, taskhead, num_classes\n",
    "\n",
    "\n",
    "def sample_batch(train_source, train_target, device):\n",
    "    x_s, labels_s = next(train_source)\n",
    "    x_t, _ = next(train_target)\n",
    "    x_s = x_s.to(device)\n",
    "    x_t = x_t.to(device)\n",
    "    labels_s = labels_s.to(device)\n",
    "    return x_s, x_t, labels_s\n",
    "\n",
    "def test_accuracy(model, loader, loss_fn, device):\n",
    "    avg_acc = 0.\n",
    "    avg_loss = 0.\n",
    "    n = len(loader.dataset)\n",
    "    model = model.to(device)\n",
    "    model = model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in tqdm(loader):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            yhat = model(x)\n",
    "            avg_loss += (loss_fn(yhat, y).item() / n)\n",
    "\n",
    "            pred = yhat.max(1, keepdim=True)[1]\n",
    "            avg_acc += (pred.eq(y.view_as(pred)).sum().item() / n)\n",
    "\n",
    "    return avg_acc, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "065811aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForeverDataIterator:\n",
    "    \"\"\"A data iterator that will never stop producing data\"\"\"\n",
    "\n",
    "    def __init__(self, data_loader: DataLoader):\n",
    "        self.data_loader = data_loader\n",
    "        self.iter = iter(self.data_loader)\n",
    "\n",
    "    def __next__(self):\n",
    "        try:\n",
    "            data = next(self.iter)\n",
    "        except StopIteration:\n",
    "            self.iter = iter(self.data_loader)\n",
    "            data = next(self.iter)\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_loader)\n",
    "    \n",
    "class fDALLearner(nn.Module):\n",
    "    def __init__(self, backbone, taskhead, taskloss, divergence, bootleneck=None, reg_coef=1, n_classes=-1,\n",
    "                 aux_head=None,\n",
    "                 grl_params=None):\n",
    "        \"\"\"\n",
    "        fDAL Learner.\n",
    "        :param backbone: z=backbone(input). Thus backbone must be nn.Module. (i.e Usually resnet without last f.c layers).\n",
    "        :param taskhead: prediction = taskhead(z). Thus taskhead must be nn.Module *(e.g The last  f.c layers of Resnet)\n",
    "        :param taskloss: the loss used to trained the model. i.e nn.CrossEntropy()\n",
    "        :param divergence: divergence name (i.e pearson, jensen).\n",
    "        :param bootleneck: (optional) a bootleneck layer after feature extractor and before the classifier.\n",
    "        :param reg_coef: the coefficient to weight the domain adaptation loss (fDAL gamma coefficient).\n",
    "        :param n_classes: if output is categorical then the number of classes. if <=1 will create a global discriminator.\n",
    "        :param aux_head: (optional) if specified with use the provided head as the domain-discriminator. If not will create it based on tashhead as described in the paper.\n",
    "        :param grl_params: dict with grl_params.\n",
    "        \"\"\"\n",
    "\n",
    "        super(fDALLearner, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        self.taskhead = taskhead\n",
    "        self.taskloss = taskloss\n",
    "        self.bootleneck = bootleneck\n",
    "        self.n_classes = n_classes\n",
    "        self.reg_coeff = reg_coef\n",
    "        self.auxhead = aux_head if aux_head is not None else self.build_aux_head_()\n",
    "\n",
    "        self.fdal_divhead = fDALDivergenceHead(divergence, self.auxhead, n_classes=self.n_classes,\n",
    "                                               grl_params=grl_params,\n",
    "                                               reg_coef=reg_coef)\n",
    "\n",
    "    def build_aux_head_(self):\n",
    "        # fDAL recommends the same architecture for both h, h'\n",
    "        auxhead = copy.deepcopy(self.taskhead)\n",
    "        if self.n_classes == -1:\n",
    "            # creates a global discriminator, fall back to DANN in most cases. useful for multihead networks.\n",
    "            aux_linear = auxhead[-1]\n",
    "            auxhead[-1] = nn.Sequential(\n",
    "                nn.Linear(aux_linear.in_features, 1)\n",
    "            )\n",
    "\n",
    "        # different initialization.\n",
    "        auxhead.apply(lambda self_: self_.reset_parameters() if hasattr(self_, 'reset_parameters') else None)\n",
    "        return auxhead\n",
    "\n",
    "    def forward(self, x, y, src_size=-1, trg_size=-1):\n",
    "        \"\"\"\n",
    "        :param x: tensor or tuple containing source and target input tensors.\n",
    "        :param y: tensor or tuple containing source and target label tensors. (if unsupervised adaptation is a tensor with labels for source)\n",
    "        :param src_size: src_size if specified. otherwise computed from input tensors\n",
    "        :param trg_size: trg_size if specified. otherwise computed from input tensors\n",
    "\n",
    "        :return: returns a tuple(tensor,dict). e.g. total_loss, {\"pred_s\": outputs_src, \"pred_t\": outputs_tgt, \"taskloss\": task_loss}\n",
    "\n",
    "        \"\"\"\n",
    "        if isinstance(x, tuple):\n",
    "            # assume x=x_source, x_target\n",
    "            src_size = x[0].shape[0]\n",
    "            trg_size = x[1].shape[0]\n",
    "            x = torch.cat((x[0], x[1]), dim=0)\n",
    "\n",
    "        y_s = y\n",
    "        y_t = None\n",
    "\n",
    "        if isinstance(y, tuple):\n",
    "            # assume y=y_source, y_target, otherwise assume y=y_source\n",
    "            # warnings.warn_explicit('using target data')\n",
    "            y_s = y[0]\n",
    "            y_t = y[1]\n",
    "\n",
    "        f = self.backbone(x)\n",
    "        f = self.bootleneck(f) if self.bootleneck is not None else f\n",
    "\n",
    "        net_output = self.taskhead(f)\n",
    "\n",
    "        # splitting source and target features\n",
    "        f_source = f.narrow(0, 0, src_size)\n",
    "        f_tgt = f.narrow(0, src_size, trg_size)\n",
    "\n",
    "        # h(g(x))\n",
    "        outputs_src = net_output.narrow(0, 0, src_size)\n",
    "        outputs_tgt = net_output.narrow(0, src_size, trg_size)\n",
    "\n",
    "        # computing losses....\n",
    "\n",
    "        # task loss in source...\n",
    "        task_loss = self.taskloss(outputs_src, y_s)\n",
    "\n",
    "        # task loss in target if labels provided. Warning!. Only on semi-sup adaptation.\n",
    "        task_loss += 0.0 if y_t is None else self.taskloss(outputs_tgt, y_t)\n",
    "\n",
    "        fdal_loss = 0.0\n",
    "        if self.reg_coeff > 0.:\n",
    "            # adaptation\n",
    "            fdal_loss = self.fdal_divhead(f_source, f_tgt, outputs_src, outputs_tgt)\n",
    "\n",
    "            # together\n",
    "            total_loss = task_loss + fdal_loss\n",
    "        else:\n",
    "            total_loss = task_loss\n",
    "\n",
    "        return total_loss, {\"pred_s\": outputs_src, \"pred_t\": outputs_tgt, \"taskloss\": task_loss, \"fdal_loss\": fdal_loss,\n",
    "                            \"fdal_src\": self.fdal_divhead.internal_stats[\"lhatsrc\"],\n",
    "                            \"fdal_trg\": self.fdal_divhead.internal_stats[\"lhattrg\"]}\n",
    "\n",
    "    def get_reusable_model(self, pack=False):\n",
    "        \"\"\"\n",
    "        Returns the usable parts of the model. For example backbone and taskhead. ignore the rest.\n",
    "\n",
    "        :param pack: if set to True. will return a model that looks like taskhead( backbone(input)). Useful for inference.\n",
    "        :return: nn.Module  or tuple of nn.Modules\n",
    "        \"\"\"\n",
    "        if pack is True:\n",
    "            return nn.Sequential(self.backbone, self.taskhead)\n",
    "        return self.backbone, self.taskhead\n",
    "\n",
    "\n",
    "class fDALDivergenceHead(nn.Module):\n",
    "    def __init__(self, divergence_name, aux_head, n_classes, grl_params=None, reg_coef=1.):\n",
    "        \"\"\"\n",
    "        :param divergence_name: divergence name (i.e pearson, jensen).\n",
    "        :param aux_head: the auxiliary head refer to paper fig 1.\n",
    "        :param n_classes:  if output is categorical then the number of classes. if <=1 will create a global discriminator.\n",
    "        :param grl_params:  dict with grl_params.\n",
    "        :param reg_coef: regularization coefficient. default 1.\n",
    "        \"\"\"\n",
    "        super(fDALDivergenceHead, self).__init__()\n",
    "        self.grl = WarmGRL(auto_step=True) if grl_params is None else WarmGRL(**grl_params)\n",
    "        self.aux_head = aux_head\n",
    "        self.fdal_loss = fDALLoss(divergence_name, gamma=1.0)\n",
    "        self.internal_stats = self.fdal_loss.internal_stats\n",
    "        self.n_classes = n_classes\n",
    "        self.reg_coef = reg_coef\n",
    "\n",
    "    def forward(self, features_s, features_t, pred_src, pred_trg) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        :param features_s: features extracted by backbone on source data.\n",
    "        :param features_t: features extracted by backbone on target data.\n",
    "        :param pred_src: prediction on src data (for classification tasks should be N,n_classes (logits))\n",
    "        :param pred_trg: prediction on trg data (for classification tasks should be N,n_classes (logits))\n",
    "        :return: fdal loss\n",
    "        \"\"\"\n",
    "\n",
    "        f = self.grl(torch.cat((features_s, features_t), dim=0))\n",
    "        src_size = features_s.shape[0]\n",
    "        trg_size = features_t.shape[0]\n",
    "\n",
    "        aux_output_f = self.aux_head(f)\n",
    "\n",
    "        # h'(g(x)) auxiliary head output on source and target respectively.\n",
    "        y_s_adv = aux_output_f.narrow(0, 0, src_size)\n",
    "        y_t_adv = aux_output_f.narrow(0, src_size, trg_size)\n",
    "\n",
    "        loss = self.fdal_loss(pred_src, pred_trg, y_s_adv, y_t_adv, self.n_classes)\n",
    "        self.internal_stats = self.fdal_loss.internal_stats  # for debugging.\n",
    "\n",
    "        return self.reg_coef * loss\n",
    "\n",
    "class GradientReverseFunction(Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx: Any, input: torch.Tensor, coeff: Optional[float] = 1.) -> torch.Tensor:\n",
    "        ctx.coeff = coeff\n",
    "        output = input * 1.0\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx: Any, grad_output: torch.Tensor) -> Tuple[torch.Tensor, Any]:\n",
    "        return grad_output.neg() * ctx.coeff, None\n",
    "\n",
    "\n",
    "class GradientReverseLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GradientReverseLayer, self).__init__()\n",
    "\n",
    "    def forward(self, *input):\n",
    "        return GradientReverseFunction.apply(*input)\n",
    "\n",
    "\n",
    "class WarmGRL(nn.Module):\n",
    "    \"\"\"Gradient Reverse Layer with warm start\n",
    "        Parameters:\n",
    "            - **alpha** (float, optional): :math:`α`. Default: 1.0\n",
    "            - **lo** (float, optional): Initial value of :math:`\\lambda`. Default: 0.0\n",
    "            - **hi** (float, optional): Final value of :math:`\\lambda`. Default: 1.0\n",
    "            - **max_iters** (int, optional): :math:`N`. Default: 1000\n",
    "            - **auto_step** (bool, optional): If True, increase :math:`i` each time `forward` is called.\n",
    "              Otherwise use function `step` to increase :math:`i`. Default: False\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(self, alpha: Optional[float] = 1.0, lo: Optional[float] = 0.0, hi: Optional[float] = 1.,\n",
    "                 max_iters: Optional[int] = 1000., auto_step: Optional[bool] = True):\n",
    "        super(WarmGRL, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.lo = lo\n",
    "        self.hi = hi\n",
    "        self.iter_num = 0\n",
    "        self.max_iters = max_iters\n",
    "        self.auto_step = auto_step\n",
    "        self.coeff_log = None\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\"\"\"\n",
    "        coeff = float(\n",
    "            2.0 * (self.hi - self.lo) / (1.0 + np.exp(-self.alpha * self.iter_num / self.max_iters))\n",
    "            - (self.hi - self.lo) + self.lo\n",
    "        )\n",
    "        if self.auto_step:\n",
    "            self.step()\n",
    "        self.coeff_log = coeff\n",
    "        return GradientReverseFunction.apply(input, coeff)\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"Increase iteration number :math:`i` by 1\"\"\"\n",
    "        self.iter_num += 1\n",
    "\n",
    "    def log_status(self):\n",
    "        params = {f'{k}': v for k, v in self.__dict__.items() if isinstance(v, (str, int, float))}\n",
    "        return params\n",
    "\n",
    "#Inspired by the work of https://arxiv.org/abs/1606.00709.\n",
    "class ConjugateDualFunction:\n",
    "\n",
    "    def __init__(self, divergence_name, gamma=4):\n",
    "        self.f_div_name = divergence_name\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def T(self, v):\n",
    "        \"\"\"T(v)\"\"\"\n",
    "\n",
    "        if self.f_div_name == \"tv\":\n",
    "            return 0.5 * torch.tanh(v)\n",
    "        elif self.f_div_name == \"kl\":\n",
    "            return v\n",
    "        elif self.f_div_name == \"klrev\":\n",
    "            return -torch.exp(v)\n",
    "        elif self.f_div_name == \"pearson\":\n",
    "            return v\n",
    "        elif self.f_div_name == \"neyman\":\n",
    "            return 1.0 - torch.exp(v)\n",
    "        elif self.f_div_name == \"hellinger\":\n",
    "            return 1.0 - torch.exp(v)\n",
    "        elif self.f_div_name == \"jensen\":\n",
    "            return log(2.0) - F.softplus(-v)\n",
    "        elif self.f_div_name == \"gammajensen\":\n",
    "            return -self.gamma * log(self.gamma) - F.softplus(-v)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown divergence.\")\n",
    "\n",
    "    def fstarT(self, v):\n",
    "        \"\"\"f^*(T(v))\"\"\"\n",
    "\n",
    "        if self.f_div_name == \"tv\":\n",
    "            return 0.5 * torch.tanh(v)\n",
    "        elif self.f_div_name == \"kl\":\n",
    "            return torch.exp(v - 1.0)\n",
    "        elif self.f_div_name == \"klrev\":\n",
    "            return -1.0 - v\n",
    "        elif self.f_div_name == \"pearson\":\n",
    "            return 0.25 * v * v + v\n",
    "        elif self.f_div_name == \"neyman\":\n",
    "            return 2.0 - 2.0 * torch.exp(0.5 * v)\n",
    "        elif self.f_div_name == \"hellinger\":\n",
    "            return torch.exp(-v) - 1.0\n",
    "        elif self.f_div_name == \"jensen\":\n",
    "            return F.softplus(v) - log(2.0)\n",
    "        elif self.f_div_name == \"gammajensen\":\n",
    "            gf = lambda v_: -self.gamma * log(self.gamma) - F.softplus(-v_)\n",
    "            return -torch.log(self.gamma + 1. - self.gamma * torch.exp(gf(v))) / self.gamma\n",
    "        else:\n",
    "            raise ValueError(\"Unknown divergence.\")\n",
    "            \n",
    "class fDALLoss(nn.Module):\n",
    "    def __init__(self, divergence_name, gamma):\n",
    "        super(fDALLoss, self).__init__()\n",
    "\n",
    "        self.lhat = None\n",
    "        self.phistar = None\n",
    "        self.phistar_gf = None\n",
    "        self.multiplier = 1.\n",
    "        self.internal_stats = {}\n",
    "        self.domain_discriminator_accuracy = -1\n",
    "\n",
    "        self.gammaw = gamma\n",
    "        self.phistar_gf = lambda t: ConjugateDualFunction(divergence_name).fstarT(t)\n",
    "        self.gf = lambda v: ConjugateDualFunction(divergence_name).T(v)\n",
    "\n",
    "    def forward(self, y_s, y_t, y_s_adv, y_t_adv, K):\n",
    "        # ---\n",
    "        #\n",
    "        #\n",
    "\n",
    "        v_s = y_s_adv\n",
    "        v_t = y_t_adv\n",
    "\n",
    "        if K > 1:\n",
    "            _, prediction_s = y_s.max(dim=1)\n",
    "            _, prediction_t = y_t.max(dim=1)\n",
    "\n",
    "            # This is not used here as a loss, it just a way to pick elements.\n",
    "\n",
    "            # picking element prediction_s k element from y_s_adv.\n",
    "            v_s = -F.nll_loss(v_s, prediction_s.detach(), reduction='none')\n",
    "            # picking element prediction_t k element from y_t_adv.\n",
    "            v_t = -F.nll_loss(v_t, prediction_t.detach(), reduction='none')\n",
    "\n",
    "        dst = self.gammaw * torch.mean(self.gf(v_s)) - torch.mean(self.phistar_gf(v_t))\n",
    "\n",
    "        self.internal_stats['lhatsrc'] = torch.mean(v_s).item()\n",
    "        self.internal_stats['lhattrg'] = torch.mean(v_t).item()\n",
    "        self.internal_stats['acc'] = self.domain_discriminator_accuracy\n",
    "        self.internal_stats['dst'] = dst.item()\n",
    "\n",
    "        # we need to negate since the obj is being minimized, so min -dst =max dst.\n",
    "        # the gradient reversar layer will take care of the rest\n",
    "        return -self.multiplier * dst\n",
    "    \n",
    "def scheduler(optimizer_, init_lr_, decay_step_, gamma_):\n",
    "    class DecayLRAfter:\n",
    "        def __init__(self, optimizer, init_lr, decay_step, gamma):\n",
    "            self.init_lr = init_lr\n",
    "            self.gamma = gamma\n",
    "            self.optimizer = optimizer\n",
    "            self.iter_num = 0\n",
    "            self.decay_step = decay_step\n",
    "\n",
    "        def get_lr(self) -> float:\n",
    "            if ((self.iter_num + 1) % self.decay_step) == 0:\n",
    "                lr = self.init_lr * self.gamma\n",
    "                self.init_lr = lr\n",
    "\n",
    "            return self.init_lr\n",
    "\n",
    "        def step(self):\n",
    "            \"\"\"Increase iteration number `i` by 1 and update learning rate in `optimizer`\"\"\"\n",
    "            lr = self.get_lr()\n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                if 'lr_mult' not in param_group:\n",
    "                    param_group['lr_mult'] = 1.\n",
    "                param_group['lr'] = lr * param_group['lr_mult']\n",
    "\n",
    "            self.iter_num += 1\n",
    "\n",
    "        def __str__(self):\n",
    "            return str(self.__dict__)\n",
    "\n",
    "    return DecayLRAfter(optimizer_, init_lr_, decay_step_, gamma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3766182",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca3c58de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main\n",
    "#\n",
    "#\n",
    "\n",
    "def main(divergence='pearson', n_epochs=30, iter_per_epoch=3000, lr=0.01, wd=0.002, reg_coef=0.5, seed=2):\n",
    "    seed_all(seed)\n",
    "\n",
    "    \"\"\"# unzip datasets if this is first run.\n",
    "    if prepare_data_if_first_time() is False:\n",
    "        return False\"\"\"\n",
    "\n",
    "    # build the network.\n",
    "    backbone, taskhead, num_classes = build_network()\n",
    "\n",
    "    # build the dataloaders.\n",
    "    train_source, train_target, test_loader = build_data_loaders()\n",
    "\n",
    "    # define the loss function....\n",
    "    taskloss = nn.CrossEntropyLoss()\n",
    "\n",
    "    # fDAL ----\n",
    "    train_target = ForeverDataIterator(train_target)\n",
    "    train_source = ForeverDataIterator(train_source)\n",
    "    learner = fDALLearner(backbone, taskhead, taskloss, divergence=divergence, reg_coef=reg_coef, n_classes=num_classes,\n",
    "                          grl_params={\"max_iters\": 3000, \"hi\": 0.6, \"auto_step\": True}  # ignore for defaults.\n",
    "                          )\n",
    "    # end fDAL---\n",
    "\n",
    "    #\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    device = torch.device('cpu')\n",
    "    learner = learner.to(device)\n",
    "\n",
    "    # define the optimizer.\n",
    "\n",
    "    # Hyperparams and scheduler follows CDAN.\n",
    "    opt = optim.SGD(learner.parameters(), lr=lr, momentum=0.9, nesterov=True, weight_decay=wd)\n",
    "    opt_schedule = scheduler(opt, lr, decay_step_=iter_per_epoch * 5, gamma_=0.5)\n",
    "\n",
    "    print('Starting training...')\n",
    "    for epochs in range(n_epochs):\n",
    "        learner.train()\n",
    "        for i in range(iter_per_epoch):\n",
    "            opt_schedule.step()\n",
    "            # batch data loading...\n",
    "            x_s, x_t, labels_s = sample_batch(train_source, train_target, device)\n",
    "            # forward and loss\n",
    "            loss, others = learner((x_s, x_t), labels_s)\n",
    "            # opt stuff\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            # avoid gradient issues if any early on training.\n",
    "            torch.nn.utils.clip_grad_norm_(learner.parameters(), 10)\n",
    "            opt.step()\n",
    "            if i % 1500 == 0:\n",
    "                print(f\"Epoch:{epochs} Iter:{i}. Task Loss:{others['taskloss']}\")\n",
    "\n",
    "        test_acc, test_loss = test_accuracy(learner.get_reusable_model(True), test_loader, taskloss, device)\n",
    "        print(f\"Epoch:{epochs} Test Acc: {test_acc} Test Loss: {test_loss}\")\n",
    "\n",
    "    # save the model.\n",
    "    torch.save(learner.get_reusable_model(True).state_dict(), './checkpoint.pt')\n",
    "    print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af59e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch:0 Iter:0. Task Loss:2.700672149658203\n",
      "Epoch:0 Iter:1500. Task Loss:0.07619177550077438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 26.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Test Acc: 0.9237668161434979 Test Loss: 0.002046589173959391\n",
      "Epoch:1 Iter:0. Task Loss:0.061489589512348175\n",
      "Epoch:1 Iter:1500. Task Loss:0.07515197992324829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 27.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1 Test Acc: 0.9436970602889887 Test Loss: 0.0015705205800821964\n",
      "Epoch:2 Iter:0. Task Loss:0.13431257009506226\n",
      "Epoch:2 Iter:1500. Task Loss:0.09628837555646896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 26.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:2 Test Acc: 0.9456900847035377 Test Loss: 0.0015278904776283321\n",
      "Epoch:3 Iter:0. Task Loss:0.0946003794670105\n",
      "Epoch:3 Iter:1500. Task Loss:0.08771919459104538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 28.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:3 Test Acc: 0.9486796213253612 Test Loss: 0.001520572207017983\n",
      "Epoch:4 Iter:0. Task Loss:0.09194111078977585\n",
      "Epoch:4 Iter:1500. Task Loss:0.050440266728401184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 28.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:4 Test Acc: 0.9511709018435476 Test Loss: 0.0014788722800388818\n",
      "Epoch:5 Iter:0. Task Loss:0.11461411416530609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a4341b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
